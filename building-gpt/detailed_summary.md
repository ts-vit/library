# Детальная выжимка: Как создать GPT с нуля (Андрей Карпати)

Пошаговый разбор легендарного курса "Zero to Hero" по созданию архитектуры Transformer (GPT-2) своими руками.

---

### Глава 1. Данные и Токенизация
*   **Сырье:** Работа с набором данных Tiny Shakespeare.
*   **Токенизация:** Почему мы не даем ИИ буквы напрямую? Перевод текста в числа (ASCII -> Индексы).
*   **Vocabulary:** Создание словаря уникальных символов (65 штук для Шекспира).

### Глава 2. Большой контекст (N-gram и Bigram)
*   **Bigram модель:** Самый простой вариант — предсказание следующего символа на основе только одного предыдущего.
*   **Обучение:** Введение понятия Loss Function (Cross Entropy) для измерения того, насколько плохо модель предсказывает символы в начале пути.

### Глава 3. Введение в Self-Attention (Само-внимание)
*   **Проблема:** Как сделать так, чтобы 5-е слово в предложении "знало" про 1-е?
*   **Механизм Query, Key, Value:**
    *   **Query (Запрос):** "Что я ищу?"
    *   **Key (Ключ):** "Что я содержу?"
    *   **Value (Значение):** "Что я передам, если меня выберут?"
*   **Масштабированное скалярное произведение:** Математический трюк для вычисления весов внимания.

### Глава 4. Построение блока Трансформера
*   **Multi-Head Attention:** Запуск нескольких процессов внимания параллельно. Один ловит грамматику, другой — смысл, третий — пунктуацию.
*   **Feed Forward Network (FFN):** Индивидуальный слой обработки для каждого токена после того, как они "пообщались" через внимание.
*   **Residual Connections:** Добавление входа к выходу слоя, чтобы предотвратить затухание градиентов при глубоком обучении.

### Глава 5. Масштабирование: От Bigram до GPT
*   **LayerNorm:** Нормализация данных внутри блоков для стабильного обучения.
*   **Dropout:** Случайное выключение нейронов для предотвращения переобучения (чтобы модель не просто зубрила, а понимала суть).
*   **Глубина:** Увеличение количества блоков (до 6-12 слоев) и размерности векторов.

### Глава 6. Генерация текста (Inference)
*   **Процесс:** Берем стартовый символ -> Предсказываем следующий -> Добавляем его в конец -> Повторяем.
*   **Температура:** Как управлять "креативностью" ИИ (высокая температура — больше случайности, низкая — более предсказуемый текст).

---
**Связь с проектом Toshik:** Понимание того, как работают Query и Key в механизме внимания, помогает нам лучше настраивать семантический поиск (RAG) в `memory_engine`. По сути, RAG — это внешний механизм внимания для нашего агента.
