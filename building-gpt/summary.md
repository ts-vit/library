# Выжимка: Как создать GPT с нуля (Let's build GPT: from scratch) - Андрей Карпати

Эта лекция является кульминацией курса «Zero to Hero». Андрей строит модель уровня GPT-2 (на 124 млн параметров) в Jupyter Notebook за 2 часа.

## Ключевые этапы построения:
1.  **Большие данные:** Используется текст Шекспира. Текст разбивается на токены (числа).
2.  **Эмбеддинги:** Каждому токену сопоставляется вектор чисел, отражающий его смысл.
3.  **Self-Attention (Само-внимание):** 
    *   Слова «общаются» друг с другом. 
    *   Каждое слово спрашивает: «Кто еще в этом предложении важен для моего смысла?». 
    *   Это позволяет ИИ понимать, что в фразе «Мальчик взял яблоко, потому что **он** был голоден», слово «он» относится к мальчику.
4.  **Multi-Head Attention:** Много слоев «внимания», которые смотрят на текст под разными углами одновременно.
5.  **Блоки Трансформера:** Комбинация внимания и полносвязных слоев. Андрей показывает, как стаканье этих блоков (12 слоев в GPT-2) создает мощный интеллект.

## Математические фишки:
*   **Softmax:** Используется для превращения оценок «важности» слов в вероятности.
*   **Residual Connections:** «Короткие замыкания» в сети, которые помогают градиентам течь глубже, не умирая.
*   **Layer Normalization:** Стабилизация обучения, чтобы числа в сети не «взрывались».

## Итог:
В конце видео маленькая модель, обученная на Шекспире, начинает писать тексты, которые выглядят как настоящий Шекспир. Это доказывает, что простая математика + много данных = способность имитировать сложный человеческий язык.
