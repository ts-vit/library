# Детальная выжимка по главам: Грокаем глубокое обучение (Эндрю Траск)

Фундаментальный разбор принципов работы нейросетей "на пальцах". Книга учит создавать ИИ с нуля на чистом Python без библиотек вроде PyTorch или TensorFlow.

---

### Глава 1. Основы: Нейронные сети и Глубокое обучение
*   **Суть:** Нейронная сеть — это просто параметрический метод приближения функций.
*   **Интуиция:** Обучение — это процесс подстройки "ручек" (весов), чтобы на выходе получить нужное число.

### Глава 2. Прогнозирование (Forward Propagation)
*   **Взвешенная сумма:** Самый простой нейрон — это вход, умноженный на вес.
*   **Сложные предсказания:** Работа с несколькими входами и выходами одновременно.
*   **Скрытые слои:** Зачем они нужны? Для поиска скрытых закономерностей, которые не видны напрямую.

### Глава 3. Оценка ошибок (Loss Functions)
*   **Mean Squared Error (MSE):** Почему мы возводим ошибку в квадрат? Чтобы избавиться от отрицательных чисел и сделать большие ошибки более "болезненными" для сети.

### Глава 4. Градиентный спуск (Обучение)
*   **Метод градиентного спуска:** Движение весов в сторону, противоположную ошибке.
*   **Альфа (Learning Rate):** Как сильно мы крутим ручки весов за один раз. Слишком быстро — проскочим минимум, слишком медленно — будем учиться вечно.

### Глава 5. Обратное распространение (Backpropagation)
*   **Связь слоев:** Как ошибка с выхода передается на скрытые слои. 
*   **Цепное правило:** Математическая основа передачи градиента через всю сеть.

### Глава 6. Регуляризация и Переобучение
*   **Проблема:** Сеть просто запоминает ответы, а не понимает логику.
*   **Dropout:** Выключение случайных нейронов. Это заставляет сеть быть более надежной и не полагаться на конкретные "пути" передачи сигнала.
*   **Batching:** Обучение на группах примеров для стабильности.

### Глава 7. Обработка естественного языка (NLP)
*   **Векторы слов (Embeddings):** Представление слов в виде набора чисел. Сходство смыслов через близость векторов.
*   **Matrix Multiplication:** Как гигантские таблицы чисел превращаются в понимание языка.

### Глава 8. Сверточные и Рекуррентные сети (Обзор)
*   **CNN:** Работа с картинками через поиск паттернов (линий, углов).
*   **RNN:** Работа с последовательностями (текст, звук), где важен порядок.

---
**Связь с проектом Toshik:** Понимание векторов и эмбеддингов из 7-й главы — это база для нашего `memory_engine`. Теперь мы знаем, почему `bge-m3` выдает именно такие цифры и как они сравниваются.
