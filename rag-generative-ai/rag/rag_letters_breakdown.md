# Детальный разбор аббревиатуры RAG (Retrieval-Augmented Generation)

Разберем архитектуру RAG по буквам, как это описывается в современной AI-инженерии (от Ротмана до Хьюен). Каждая буква — это отдельный технический слой со своими вызовами.

---

## **R — Retrieval (Извлечение)**
Это "ноги" системы. Модель не лезет в свои веса, а идет во внешнее хранилище за фактами.

**Что происходит на этом этапе:**
1.  **Запрос (Query):** Вопрос пользователя превращается в вектор с помощью той же модели-эмбеддера (например, **BGE-M3**), которой мы индексировали базу.
2.  **Поиск (Search):** Векторная БД (Pinecone, Chroma, SQLite+Vector) выполняет поиск "ближайших соседей".
3.  **Гибридность:** Хороший Retrieval сочетает в себе:
    *   **Векторный поиск (Dense):** Находит "смысл" (машина == автомобиль).
    *   **Полнотекстовый поиск (Sparse/BM25):** Находит точные совпадения имен, кодов ошибок, артикулов.
4.  **Re-ranking (Переранжирование):** Из найденных 50 чанков специальная "тяжелая" модель-реранкер выбирает топ-5 самых полезных.

**Проблема:** Если Retrieval найдет плохие данные ("мусор на входе"), всё остальное бессмысленно.

---

## **A — Augmented (Дополнение / Обогащение)**
Это "клей" системы. Здесь мы объединяем вопрос пользователя с найденными данными.

**Что происходит на этом этапе:**
1.  **Сборка контекста:** Мы берем текст чанков, найденных на шаге R, и вставляем их в специальный шаблон (Prompt Template).
2.  **Управление контекстным окном:** Если данных слишком много, мы должны решить, что оставить, а что выкинуть, чтобы не превысить лимит токенов и не сделать запрос слишком дорогим.
3.  **Инструкции (System Prompt):** Мы "накачиваем" (augment) запрос строгими правилами:
    *   "Используй ТОЛЬКО предоставленный текст".
    *   "Если в тексте нет ответа, не придумывай его".
    *   "Цитируй источники (например, название файла или номер страницы)".

**Суть:** На этом этапе мы превращаем абстрактный вопрос в "закрытую задачу" для ИИ, где все ответы лежат перед его глазами.

---

## **G — Generation (Генерация)**
Это "голос" системы. Здесь вступает в дело сама большая языковая модель (LLM).

**Что происходит на этом этапе:**
1.  **Рассуждение (Reasoning):** Модель (например, Gemini 3 Flash или Pro) читает обогащенный промт, сопоставляет вопрос с контекстом и формулирует связный ответ.
2.  **Синтез:** Модель не просто копирует текст из базы, она перефразирует его под запрос пользователя.
3.  **Форматирование:** Мы просим модель выдать ответ в нужном нам виде: JSON для API, Markdown для чата или короткое "Да/Нет".
4.  **Контроль галлюцинаций:** Поскольку у модели есть контекст, её склонность выдумывать факты снижается на 90-95%.

---

### **Итоговая формула:**
*   **R** — Найди нужные документы в базе.
*   **A** — Подмешай их в промт вместе с вопросом и правилами.
*   **G** — Сгенерируй на этой основе понятный и точный ответ.

**Пример:**
*   **R:** Нашёл в инструкции к кофемашине главу про "Ошибка E01".
*   **A:** Написал в промт: "Вот инструкция (глава про E01). Пользователь спрашивает 'что делать, если мигает E01?'. Ответь кратко".
*   **G:** Модель выдаёт: "Вам нужно очистить контейнер для жмыха, как указано в инструкции".
