# RAG: Продвинутая архитектура и лучшие практики (2026)

Этот документ представляет собой глубокое погружение в проектирование систем **Retrieval-Augmented Generation (RAG)**, объединяя знания из книги Дэниса Ротмана, опыт Чип Хьюен и актуальные технологические тренды начала 2026 года.

---

### 1. Почему RAG — это стандарт индустрии?
LLM имеют две фундаментальные проблемы: **знания ограничены датой обучения** и **отсутствие доступа к приватным данным**. RAG решает их, превращая модель из "базы данных" в "движок рассуждений" (reasoning engine), работающий поверх ваших актуальных данных.

### 2. Пайплайн RAG: Анатомия и тонкая настройка

#### А. Индексация (Ingestion) — фундамент качества
Здесь мы подготавливаем знания. В 2026 году мы уходим от простого разбиения текста к "умному" чанкингу:
*   **Semantic Chunking:** Разбиение не по количеству знаков, а по смысловым паузам (параграфы, логические блоки).
*   **Hierarchical Chunking:** Создание маленьких чанков для точного поиска и больших родительских чанков для контекста.
*   **Модели эмбеддингов:** Использование мощных локальных моделей типа **BGE-M3** (8k контекста, гибридный поиск).

#### Б. Retrieval (Извлечение) — поиск иглы в стоге сена
Одного векторного поиска часто недостаточно. Лучшая практика сегодня — **Гибридный поиск**:
1.  **Dense Retrieval (Векторный):** Понимает синонимы и смысл ("автомобиль" == "машина").
2.  **Sparse Retrieval (Ключевые слова):** Находит точные термины, артикулы и имена (BM25).
3.  **Reranking (Переранжирование):** Мы берем топ-50 результатов гибридного поиска и прогоняем их через модель-реранкер (например, BGE-Reranker), которая расставляет их по реальной релевантности. Именно это убивает галлюцинации.

#### В. Generation (Ответ) — контроль качества
LLM получает промт с инструкцией: *"Отвечай только на основе контекста. Если ответа нет — скажи 'не знаю'"*. Для сложных систем используется **Adaptive RAG**, когда модель сама решает, достаточно ли ей данных или нужно сходить в поиск еще раз.

---

### 3. Оценка системы (Evaluation)
Как понять, что ваш RAG не врет? Мы используем метрики **RAGAS**:
1.  **Faithfulness (Верность):** Можно ли подтвердить ответ фактами из найденного контекста?
2.  **Answer Relevance (Релевантность):** Насколько ответ соответствует вопросу пользователя?
3.  **Context Precision (Точность контекста):** Оказался ли нужный ответ в топе найденных чанков?

### 4. Рекомендации для наших проектов (toshik-babe)
Для бэкенда `toshik-babe` на основе книги Чип Хьюен и текущих наработок:
*   **Локальный поиск:** Перейти на **BGE-M3** для полной поддержки русского языка и длинных документов (уже реализовано в нашем `memory_engine`).
*   **LLM-as-a-judge:** Использовать Gemini 1.5 Pro для автоматической проверки ответов Flash-модели на "галлюцинации" (Faithfulness).
*   **SQL + Vector:** Использовать SQLite не только для логов, но и для метаданных к векторам (фильтрация по дате, пользователю, проекту).

---

### Итог для разработчика
RAG — это не "просто промт с текстом". Это **система управления знаниями**. Ваша задача как инженера — обеспечить, чтобы в 100% случаев модель получала именно те 3-5 кусочков текста, в которых спрятан ответ. Если Retrieval отработал идеально, даже маленькая модель (Gemini Flash или GPT-4o-mini) ответит как эксперт.
